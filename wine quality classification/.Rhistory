white <- read.csv("./winequality-white.csv",header=TRUE, sep=";")
white$good.wine <- ifelse(white$quality>=6,1,0)
set.seed(42)
n <- nrow(white)
shuffled_df <- white[sample(n), ]
train_indices <- 1:round(0.4 * n)
train <- shuffled_df[train_indices, ]
vali_indices <- (round(0.4 * n) + 1):(round(0.7*n))
validation <- shuffled_df[vali_indices, ]
test_indices <- (round(0.7*n)+1):n
test <- shuffled_df[test_indices, ]
train_z <- as.data.frame(scale(train[,1:(ncol(train)-2)]))
train_label <- train$good.wine
#use mean and sd of training data on validation and test data
train_mean <- apply(train[,1:(ncol(train)-2)],2,mean)
train_sd <- apply(train[,1:(ncol(train)-2)],2,sd)
validation_z <- as.data.frame(scale(validation[,1:(ncol(validation)-2)],train_mean, train_sd))
validation_label <- validation$good.wine
test_z <- as.data.frame(scale(test[,1:(ncol(test)-2)],train_mean, train_sd))
test_label <- test$good.wine
library(class)
library(caret)
knn_train <- function(k) {
pred <-  knn(train=train_z, test=validation_z,
cl=train_label, k=k)
return (pred)
}
# try recording all common evaluation metrics
k_list <- c()
accuracy_list <- c()
precision_list <- c()
recall_list <- c()
f1_list <- c()
for (k in 1:80) {
pred <- knn_train(k)
assign(paste("pred",k,sep="_"), pred)
k_list <- c(k_list, k)
accuracy <- mean(pred==validation_label)
accuracy_list <- c(accuracy_list, accuracy)
precision <- posPredValue(pred, as.factor(validation_label), positive = "1")
precision_list <- c(precision_list, precision)
recall <- sensitivity(pred, as.factor(validation_label), positive="1")
recall_list <- c(recall_list, recall)
f1 <- (2 * precision * recall) / (precision + recall)
f1_list <- c(f1_list, f1)
}
performance <- data.frame(k_list, accuracy_list, precision_list, recall_list, f1_list)
sorted_perf <- performance[order(-accuracy_list),]
sorted_perf[1,]
pred_test <-  knn(train=train_z, test=test_z, cl=train_label, k=15)
#confusion matrix
cm <- table("Actual"=test_label, "Prediction"=pred_test)
library(knitr)
kable(cm, caption = "Confusion Matrix")
#generalisation error - using misclassfication rate
g_error <- mean(pred_test!=test_label)
g_error
pred_test <-  knn(train=train_z, test=test_z, cl=train_label, k=15)
